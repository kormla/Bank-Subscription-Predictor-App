{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffcbad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import flask\n",
    "from flask import request, jsonify\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19044832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and features loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Flask app\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "# --- Load the trained model and features ---\n",
    "# Make sure these paths are correct relative to your app.py\n",
    "try:\n",
    "    model = joblib.load('random_forest_model.pkl')\n",
    "    # Load the list of expected feature columns from training\n",
    "    model_features = joblib.load('model_features.pkl')\n",
    "    print(\"Model and features loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or features: {e}\")\n",
    "    model = None\n",
    "    model_features = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4606020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return \"Hello, this is my prediction app!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a224ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Preprocessing Function (Crucial for consistent predictions) ---\n",
    "def preprocess_input(data: dict, model_features: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies the same preprocessing steps as performed during model training.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the input dictionary\n",
    "    df_input = pd.DataFrame([data])\n",
    "\n",
    "    # --- 1. Replicate 'unknown' handling (if applicable for new inputs) ---\n",
    "    # In a real API, you might validate input or assume clean input.\n",
    "    # For now, we'll assume 'unknown' are not directly sent, or need specific handling.\n",
    "    # If your model needs to handle 'unknown' in new data, you'd replicate that logic here.\n",
    "    # For simplicity, we assume the input dictionary already reflects cleaned categorical values.\n",
    "\n",
    "    # --- 2. Replicate dropping 'duration' (if present in input) ---\n",
    "    if 'duration' in df_input.columns:\n",
    "        df_input = df_input.drop('duration', axis=1)\n",
    "\n",
    "    # --- 3. Replicate Feature Engineering ---\n",
    "    if 'pdays' in df_input.columns:\n",
    "        df_input['was_contacted_before'] = (df_input['pdays'] != -1).astype(int)\n",
    "    else:\n",
    "        # Handle case where pdays might be missing in new input (e.g., default to 0)\n",
    "        df_input['was_contacted_before'] = 0\n",
    "\n",
    "    if 'campaign' in df_input.columns:\n",
    "        df_input['multiple_campaign_contacts'] = (df_input['campaign'] > 1).astype(int)\n",
    "    else:\n",
    "        # Handle case where campaign might be missing in new input\n",
    "        df_input['multiple_campaign_contacts'] = 0\n",
    "\n",
    "    # --- 4. Replicate Outlier Handling (Capping) ---\n",
    "    numerical_cols_for_outlier_handling = ['balance', 'campaign', 'pdays', 'previous']\n",
    "    for col in numerical_cols_for_outlier_handling:\n",
    "        if col in df_input.columns and col in model_features: # Check if col exists and was processed during training\n",
    "            # You need the Q1, Q3, IQR from your TRAINING DATA for capping.\n",
    "            # For simplicity, we'll hardcode or fetch them if saved.\n",
    "            # A robust solution would save these bounds alongside the model.\n",
    "            # Example hardcoded (replace with actual values from your training data's IQR calculation):\n",
    "            if col == 'balance':\n",
    "                lower_bound = -309.00 # Example, replace with actual\n",
    "                upper_bound = 1589.00 # Example, replace with actual\n",
    "            elif col == 'campaign':\n",
    "                lower_bound = -2.00 # Example, replace with actual\n",
    "                upper_bound = 7.00 # Example, replace with actual\n",
    "            elif col == 'pdays':\n",
    "                lower_bound = -266.50 # Example, replace with actual\n",
    "                upper_bound = 639.50 # Example, replace with actual\n",
    "            elif col == 'previous':\n",
    "                lower_bound = -1.50 # Example, replace with actual\n",
    "                upper_bound = 2.50 # Example, replace with actual\n",
    "            else:\n",
    "                continue # Skip if no bounds defined\n",
    "\n",
    "            df_input[col] = np.where(df_input[col] < lower_bound, lower_bound, df_input[col])\n",
    "            df_input[col] = np.where(df_input[col] > upper_bound, upper_bound, df_input[col])\n",
    "\n",
    "\n",
    "    # --- 5. Replicate One-Hot Encoding ---\n",
    "    # This is the most complex part with pd.get_dummies.\n",
    "    # You need to ensure the new input DataFrame has ALL columns (including dummy columns)\n",
    "    # that your model was trained on, and in the correct order.\n",
    "    # Missing columns must be added with 0, and extra columns dropped.\n",
    "\n",
    "    # Identify categorical columns that were originally one-hot encoded\n",
    "    # You MUST get this list from your training script.\n",
    "    # Example (replace with your actual list after dropping 'duration' and 'y'):\n",
    "    original_categorical_cols = [\n",
    "        'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n",
    "        'month', 'poutcome' # Assuming these were present after 'unknown' handling\n",
    "    ]\n",
    "\n",
    "    # Convert object columns in current input to category type for consistency with get_dummies\n",
    "    for col in original_categorical_cols:\n",
    "        if col in df_input.columns:\n",
    "            # Ensure the categories are known for new data.\n",
    "            # This is safer if you save the categories from training data.\n",
    "            # For simplicity, we let get_dummies infer, but this is less robust.\n",
    "            df_input[col] = df_input[col].astype('category')\n",
    "\n",
    "\n",
    "    # Apply one-hot encoding to the input DataFrame\n",
    "    df_processed = pd.get_dummies(df_input, columns=original_categorical_cols, drop_first=True)\n",
    "\n",
    "\n",
    "    # Align columns with the model's training features (CRITICAL STEP)\n",
    "    # This ensures that your input DataFrame for prediction has the exact same columns\n",
    "    # in the exact same order as your training data.\n",
    "    missing_cols = set(model_features) - set(df_processed.columns)\n",
    "    for c in missing_cols:\n",
    "        df_processed[c] = 0 # Add missing dummy variables as 0\n",
    "\n",
    "    # Ensure the order of columns is the same as the training data\n",
    "    df_final = df_processed[model_features]\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b52cfb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- API Endpoint for Prediction ---\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if model is None or model_features is None:\n",
    "        return jsonify({'error': 'Model or features not loaded. Check server logs.'}), 500\n",
    "\n",
    "    try:\n",
    "        # Get data from the POST request\n",
    "        data = request.get_json(force=True)\n",
    "        if not data:\n",
    "            return jsonify({'error': 'No JSON data provided.'}), 400\n",
    "\n",
    "        # Preprocess the input data\n",
    "        processed_data = preprocess_input(data, model_features)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction_proba = model.predict_proba(processed_data)[:, 1][0] # Probability of 'yes'\n",
    "        prediction = model.predict(processed_data)[0] # 0 or 1\n",
    "\n",
    "        # Return the prediction as JSON\n",
    "        result = {\n",
    "            'prediction': int(prediction), # Convert numpy int to Python int for JSON\n",
    "            'probability_of_subscription': float(prediction_proba) # Convert numpy float to Python float\n",
    "        }\n",
    "        return jsonify(result), 200\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any errors and return them\n",
    "        return jsonify({'error': str(e), 'trace': traceback.format_exc()}), 500\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b693cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Health Check Endpoint (Optional but good practice) ---\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({'status': 'healthy', 'model_loaded': model is not None}), 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "929132b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "# --- Main entry point for Heroku ---\n",
    "if __name__ == '__main__':\n",
    "    # For local testing:\n",
    "    # Ensure you have your model and model_features.pkl in the same directory\n",
    "    # Run with: python app.py\n",
    "    # Test with:\n",
    "    # curl -X POST -H \"Content-Type: application/json\" -d '{\"age\":30, \"job\":\"unemployed\", \"marital\":\"married\", \"education\":\"primary\", \"default\":\"no\", \"balance\":1787, \"housing\":\"no\", \"loan\":\"no\", \"contact\":\"cellular\", \"day\":19, \"month\":\"oct\", \"campaign\":1, \"pdays\":-1, \"previous\":0, \"poutcome\":\"unknown\"}' http://127.0.0.1:5000/predict\n",
    "    app.run(debug=True, host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c074d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59636ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5934b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
